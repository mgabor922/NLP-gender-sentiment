{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation , Embedding , Flatten\n",
    "from keras.callbacks import Callback\n",
    "from keras.optimizers import Adam\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "# right now we only take 10k samples to make the testing of the code faster\n",
    "sample_size = 50000\n",
    "# this is the maximum number of words we take from the blogs\n",
    "max_length = 500\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def get_word_dict(docs_to_process):\n",
    "    word_dict = {}\n",
    "    for text in docs_to_process:\n",
    "        words = re.findall(r\"[\\w']+\", text)\n",
    "        for word in words:\n",
    "            if word in word_dict:\n",
    "                word_dict[word] = word_dict[word] + 1\n",
    "            else:\n",
    "                word_dict[word] = 1\n",
    "      \n",
    "    return word_dict\n",
    "\n",
    "def make_gender_list(gender_list):\n",
    "  gender_list_binary = np.zeros(gender_list.shape[0])\n",
    "  print(\"gender list shape: \" + str(gender_list_binary.shape))\n",
    "  for i in range(len(gender_list_binary)):\n",
    "    if(gender_list[i] == 'male'):\n",
    "      gender_list_binary[i] = 1\n",
    "  return gender_list_binary\n",
    "\n",
    "\n",
    "#This function removes stopwords and also words that appear infrequently, and it cuts blogposts longer than 500 words.\n",
    "def reduce_vocab(docs, word_limit, sparsewords=False):\n",
    "  for i,  blogpost in enumerate(docs):\n",
    "    word_tokens = word_tokenize(docs[i])\n",
    "    \n",
    "    if(len(word_tokens) > word_limit):\n",
    "      word_tokens = word_tokens[0:word_limit]\n",
    "    \n",
    "    blogpost_reduced = [w for w in word_tokens if not w in stop_words]\n",
    "    # WORD STEMMING:\n",
    "    blogpost_reduced = [ps.stem(w) for w in blogpost_reduced]\n",
    "    \n",
    "    if(sparsewords == True):\n",
    "      blogpost_reduced = [w for w in blogpost_reduced if not w in infrequent_words ]\n",
    "      \n",
    "    docs[i] = ' '.join( blogpost_reduced ) #.replace(' , ',',').replace(' .','.').replace(' !','!').replace(' ?','?')\n",
    "\n",
    "\n",
    "df = pd.read_json(\"data.json\")\n",
    "df.head()\n",
    "\n",
    "stop_words = set(stopwords.words('english'))    #set of stopwords\n",
    "print(stop_words)\n",
    "\n",
    "\n",
    "# get sample_size amount of data from the database\n",
    "values = df.values[0:sample_size]\n",
    "print(values.shape)\n",
    "#print(values[0:2])\n",
    "\n",
    "docs = values[: , 2]\n",
    "#print(docs.shape)\n",
    "labels = values[: , 0:2]\n",
    "#print(docs[0:5])\n",
    "#print(labels[0:5][:])\n",
    "\n",
    "#change database to lower case letters\n",
    "for i,doc in enumerate(docs):\n",
    "  docs[i] = doc.lower()\n",
    "\n",
    "\n",
    "# Here we create a word dictionary of the blog posts, to see how many different\n",
    "# words are in them, to decide what the vocab_size should be.\n",
    "# We also check how many words we might not even need, because they appear very\n",
    "# infrequently.\n",
    "# We also use this to create a list of the infrequent words, so we can remove them,\n",
    "# to see if it helps later in the modelling phase.\n",
    "\n",
    "\n",
    "docs_to_process = docs\n",
    "\n",
    "  \n",
    "word_dict = get_word_dict(docs_to_process)\n",
    "\n",
    "infrequent_words = []\n",
    "\n",
    "word_dict_small = word_dict.copy()\n",
    "for elem in word_dict:\n",
    "    if word_dict[elem] <= 1:\n",
    "      infrequent_words.append(elem)\n",
    "      del word_dict_small[elem]\n",
    "      \n",
    "print(\"Size of the word dictionary: \" + str(len(word_dict)))\n",
    "print(\"Size without infrequent words: \" + str(len(word_dict_small)))\n",
    "#print(infrequent_words)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "# Right now the removal of sparse/infrequent words is slow, we might have to find a different way to reduce vocabulary size\n",
    "reduce_vocab(docs, max_length, sparsewords=False)\n",
    "gender_list_binary = make_gender_list(labels[:,1])\n",
    "\n",
    "\n",
    "final_wd = get_word_dict(docs)\n",
    "vocab_size = len(docs)\n",
    "encoded_docs = [keras.preprocessing.text.one_hot(d, vocab_size , filters='') for d in docs]\n",
    "padded_docs = keras.preprocessing.sequence.pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "\n",
    "padded_docs_unlistified = list()\n",
    "for i in range(padded_docs.shape[0]):\n",
    "    padded_docs_unlistified.append(','.join(str(x) for x in padded_docs[i]))\n",
    "# check is the the first list and string are the same\n",
    "      \n",
    "\n",
    "dataout = {'age' : labels[:,0] , 'gender': gender_list_binary , 'post': padded_docs_unlistified}\n",
    "\n",
    "dfout = pd.DataFrame( data=dataout ) \n",
    "dfout.to_json('out2.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
