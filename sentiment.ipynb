{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I tried the airline sentiment database again with a convolutional netfwork after the embedding layer\n",
    "instead of lstm.\n",
    "LINK:\n",
    "https://data.world/crowdflower/airline-twitter-sentiment\n",
    "this database ocntains neutral, positive and negative sentiments regarding an airline in tweets\n",
    "important columns: airline_sentiment, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic packages\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import re\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Packages for data preparation\n",
    "from keras import optimizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing import sequence \n",
    "from keras.datasets import imdb \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation ,Dropout, Embedding , Flatten, LSTM, SpatialDropout1D,Convolution1D\n",
    "from keras.callbacks import Callback\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Packages for modeling\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import regularizers\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "NB_START_EPOCHS = 20  # Number of epochs we usually start to train with\n",
    "BATCH_SIZE = 16  # Size of the batches used in the mini-batch gradient descent\n",
    "PADDED_SIZE = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of docs: 14640\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>airline_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10723</th>\n",
       "      <td>Now next plane broken seat! My row! Another h...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8878</th>\n",
       "      <td>Airways Short Interest Down 3.5% January (JBL...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12059</th>\n",
       "      <td>flight got Cancelled Flightled GRK DFW, LEX t...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1256</th>\n",
       "      <td>already did.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4229</th>\n",
       "      <td>I'm not person, I've sitting Denver three hou...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text airline_sentiment\n",
       "10723   Now next plane broken seat! My row! Another h...          negative\n",
       "8878    Airways Short Interest Down 3.5% January (JBL...           neutral\n",
       "12059   flight got Cancelled Flightled GRK DFW, LEX t...           neutral\n",
       "1256                                        already did.           neutral\n",
       "4229    I'm not person, I've sitting Denver three hou...          negative"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Tweets.csv', encoding = \"ISO-8859-1\")\n",
    "df = df.reindex(np.random.permutation(df.index))  \n",
    "df = df[['text', 'airline_sentiment']]\n",
    "\n",
    "\n",
    "# remove neutral sentiment if needed\n",
    "#df = df[df.sentiment != \"neutral\"]\n",
    "\n",
    "# removing stop words like 'the, a, etc' but whitelisting ones that could be useful for sentiment\n",
    "# analysis (not, no) could indicate negative emotions for example\n",
    "def remove_stopwords(input_text):\n",
    "        stopwords_list = stopwords.words('english')\n",
    "        # Some words which might indicate a certain sentiment are kept via a whitelist\n",
    "        whitelist = [\"n't\", \"not\", \"no\"]\n",
    "        words = input_text.split() \n",
    "        clean_words = [word for word in words if (word not in stopwords_list or word in whitelist) and len(word) > 1] \n",
    "        return \" \".join(clean_words) \n",
    "    \n",
    "def remove_mentions(input_text):\n",
    "        return re.sub(r'@\\w+', '', input_text)\n",
    "       \n",
    "df.text = df.text.apply(remove_stopwords).apply(remove_mentions)\n",
    "\n",
    "\n",
    "docs_string = df.text #df.values[:,0]#df['text']#df.values[:,0]\n",
    "sentiments_string = df.airline_sentiment#df.values[:,1]#df.airline_sentiment\n",
    "print(\"len of docs: \" + str(len(docs_string)))\n",
    "\n",
    "\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB_WORDS: 14932\n",
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# using pre trained embedding layer\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(docs_string)\n",
    "#calculate the word dictionary size, this is what we one-hot our data by\n",
    "NB_WORDS = len(t.word_index) + 1\n",
    "print(\"NB_WORDS: \" + str(NB_WORDS))\n",
    "embeddings_index = dict()\n",
    "f = open('glove.6B.100d.txt', encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = np.zeros((NB_WORDS, 100))\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length of docs is:\n",
      "26\n",
      "shape of X_train\n",
      "(10248, 30)\n",
      "shape of Y_train\n",
      "(10248, 3)\n",
      "shape of X_valid\n",
      "(3074, 30)\n",
      "shape of Y_valid\n",
      "(3074, 3)\n",
      "shape of X_test\n",
      "(1318, 30)\n",
      "shape of Y_test\n",
      "(1318, 3)\n"
     ]
    }
   ],
   "source": [
    "docs_encoded = [one_hot(d,\n",
    "                        NB_WORDS,\n",
    "                        filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                        lower=True) for d in docs_string]\n",
    "docs_encoded = np.asarray(docs_encoded)\n",
    "docs_encoded.shape\n",
    "print(\"max length of docs is:\")\n",
    "print(len(max(docs_encoded, key=len)))\n",
    "    \n",
    "X = sequence.pad_sequences(docs_encoded, maxlen=PADDED_SIZE)\n",
    "# Encode labels to first numbers than one-hot\n",
    "le = LabelEncoder()\n",
    "\n",
    "sentiments_le = le.fit_transform(sentiments_string)\n",
    "Y = to_categorical(sentiments_le)\n",
    "#get number of output classes, this is how many nodes the output layer will require\n",
    "class_num = len(le.classes_)\n",
    "X_train, X_valid_test, Y_train, Y_valid_test = train_test_split(X,Y, test_size = 0.3, random_state = 37)\n",
    "X_valid, X_test, Y_valid, Y_test = train_test_split(X_valid_test,Y_valid_test, test_size = 0.3, random_state = 37)\n",
    "print(\"shape of X_train\")\n",
    "print(X_train.shape)\n",
    "print(\"shape of Y_train\")\n",
    "print(Y_train.shape)\n",
    "print(\"shape of X_valid\")\n",
    "print(X_valid.shape)\n",
    "print(\"shape of Y_valid\")\n",
    "print(Y_valid.shape)\n",
    "print(\"shape of X_test\")\n",
    "print(X_test.shape)\n",
    "print(\"shape of Y_test\")\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "patience=15\n",
    "early_stopping=EarlyStopping(patience=patience, verbose=1)\n",
    "checkpointer=ModelCheckpoint(filepath='weights.hdf5', save_best_only=True, verbose=1)\n",
    "checkpointer_LSTM=ModelCheckpoint(filepath='weights_LSTM.hdf5', save_best_only=True, verbose=1)\n",
    "checkpointer_LSTM_glove=ModelCheckpoint(filepath='weights_LSTM_glove.hdf5', save_best_only=True, verbose=1)\n",
    "checkpointer_CNN=ModelCheckpoint(filepath='weights_CNN.hdf5', save_best_only=True, verbose=1)\n",
    "checkpointer_CNN_glove=ModelCheckpoint(filepath='weights_CNN_glove.hdf5', save_best_only=True, verbose=1)\n",
    "\n",
    "#TrainingHistory to get training logs from callback\n",
    "class TrainingHistory(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "        self.valid_losses = []\n",
    "        self.accs = [] \n",
    "        self.valid_accs = []\n",
    "        self.epoch = 0\n",
    "    \n",
    "    # After every epoch append new values to the log arrays\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.valid_losses.append(logs.get('val_loss'))\n",
    "        self.accs.append(logs.get('acc'))\n",
    "        self.valid_accs.append(logs.get('val_acc'))\n",
    "        self.epoch += 1\n",
    "            \n",
    "history = TrainingHistory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Model without GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_26 (Embedding)     (None, 30, 100)           1493200   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_22 (Spatia (None, 30, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_26 (LSTM)               (None, 200)               240800    \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 3)                 603       \n",
      "=================================================================\n",
      "Total params: 1,734,603\n",
      "Trainable params: 1,734,603\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 10248 samples, validate on 3074 samples\n",
      "Epoch 1/15\n",
      "10248/10248 [==============================] - 43s 4ms/step - loss: 0.7157 - acc: 0.7032 - val_loss: 0.5624 - val_acc: 0.7729\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.52660\n",
      "Epoch 2/15\n",
      "10248/10248 [==============================] - 39s 4ms/step - loss: 0.5240 - acc: 0.7964 - val_loss: 0.5124 - val_acc: 0.7921\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.52660 to 0.51237, saving model to weights_LSTM.hdf5\n",
      "Epoch 3/15\n",
      "10248/10248 [==============================] - 38s 4ms/step - loss: 0.4160 - acc: 0.8406 - val_loss: 0.5382 - val_acc: 0.7843\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.51237\n",
      "Epoch 4/15\n",
      "10248/10248 [==============================] - 36s 4ms/step - loss: 0.3457 - acc: 0.8704 - val_loss: 0.5662 - val_acc: 0.7827\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.51237\n",
      "Epoch 5/15\n",
      "10248/10248 [==============================] - 37s 4ms/step - loss: 0.2971 - acc: 0.8916 - val_loss: 0.6003 - val_acc: 0.7798\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.51237\n",
      "Epoch 6/15\n",
      "10248/10248 [==============================] - 37s 4ms/step - loss: 0.2604 - acc: 0.9047 - val_loss: 0.6563 - val_acc: 0.7739\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.51237\n",
      "Epoch 7/15\n",
      "10248/10248 [==============================] - 35s 3ms/step - loss: 0.2212 - acc: 0.9181 - val_loss: 0.6751 - val_acc: 0.7746\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.51237\n",
      "Epoch 8/15\n",
      "10248/10248 [==============================] - 33s 3ms/step - loss: 0.2009 - acc: 0.9283 - val_loss: 0.7298 - val_acc: 0.7739\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.51237\n",
      "Epoch 9/15\n",
      "10248/10248 [==============================] - 31s 3ms/step - loss: 0.1772 - acc: 0.9345 - val_loss: 0.7473 - val_acc: 0.7716\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.51237\n",
      "Epoch 10/15\n",
      "10248/10248 [==============================] - 31s 3ms/step - loss: 0.1559 - acc: 0.9426 - val_loss: 0.7726 - val_acc: 0.7710\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.51237\n",
      "Epoch 11/15\n",
      "10248/10248 [==============================] - 31s 3ms/step - loss: 0.1447 - acc: 0.9463 - val_loss: 0.8966 - val_acc: 0.7716\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.51237\n",
      "Epoch 12/15\n",
      "10248/10248 [==============================] - 30s 3ms/step - loss: 0.1273 - acc: 0.9562 - val_loss: 0.8783 - val_acc: 0.7720\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.51237\n",
      "Epoch 13/15\n",
      "10248/10248 [==============================] - 30s 3ms/step - loss: 0.1241 - acc: 0.9558 - val_loss: 1.0051 - val_acc: 0.7638\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.51237\n",
      "Epoch 14/15\n",
      "10248/10248 [==============================] - 30s 3ms/step - loss: 0.1068 - acc: 0.9623 - val_loss: 1.0445 - val_acc: 0.7619\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.51237\n",
      "Epoch 15/15\n",
      "10248/10248 [==============================] - 31s 3ms/step - loss: 0.1062 - acc: 0.9613 - val_loss: 1.0058 - val_acc: 0.7609\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.51237\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2469a943f28>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "embedding_vector_length = 100 \n",
    "model = Sequential() \n",
    "model.add(Embedding(NB_WORDS, embedding_vector_length, input_length=PADDED_SIZE)) \n",
    "model.add(SpatialDropout1D(0.5))\n",
    "model.add(LSTM(200, dropout=0.4, recurrent_dropout=0.4))\n",
    "#model.add(Flatten())\n",
    "model.add(Dense(class_num, activation='softmax')) \n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy']) \n",
    "print(model.summary())\n",
    "model.fit(X_train,\n",
    "          Y_train,\n",
    "          validation_data=(X_valid, Y_valid),\n",
    "          epochs=15,\n",
    "          batch_size=BATCH_SIZE,\n",
    "         callbacks=[checkpointer_LSTM, early_stopping, history])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Model with GloVE (pre trained embedding layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_32 (Embedding)     (None, 30, 100)           1493200   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_28 (Spatia (None, 30, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_32 (LSTM)               (None, 200)               240800    \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 3)                 603       \n",
      "=================================================================\n",
      "Total params: 1,734,603\n",
      "Trainable params: 1,734,603\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 10248 samples, validate on 3074 samples\n",
      "Epoch 1/15\n",
      "10248/10248 [==============================] - 36s 4ms/step - loss: 0.8208 - acc: 0.6503 - val_loss: 0.7061 - val_acc: 0.7121\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.68865\n",
      "Epoch 2/15\n",
      "10248/10248 [==============================] - 32s 3ms/step - loss: 0.6968 - acc: 0.7105 - val_loss: 0.6147 - val_acc: 0.7482\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.68865 to 0.61466, saving model to weights_LSTM_glove.hdf5\n",
      "Epoch 3/15\n",
      "10248/10248 [==============================] - 33s 3ms/step - loss: 0.6202 - acc: 0.7438 - val_loss: 0.5636 - val_acc: 0.7645\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.61466 to 0.56365, saving model to weights_LSTM_glove.hdf5\n",
      "Epoch 4/15\n",
      "10248/10248 [==============================] - 33s 3ms/step - loss: 0.5563 - acc: 0.7715 - val_loss: 0.5580 - val_acc: 0.7824\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.56365 to 0.55802, saving model to weights_LSTM_glove.hdf5\n",
      "Epoch 5/15\n",
      "10248/10248 [==============================] - 33s 3ms/step - loss: 0.5079 - acc: 0.7988 - val_loss: 0.5156 - val_acc: 0.7912\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.55802 to 0.51559, saving model to weights_LSTM_glove.hdf5\n",
      "Epoch 6/15\n",
      "10248/10248 [==============================] - 33s 3ms/step - loss: 0.4595 - acc: 0.8164 - val_loss: 0.5186 - val_acc: 0.7983\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.51559\n",
      "Epoch 7/15\n",
      "10248/10248 [==============================] - 33s 3ms/step - loss: 0.4269 - acc: 0.8324 - val_loss: 0.5298 - val_acc: 0.7983\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.51559\n",
      "Epoch 8/15\n",
      "10248/10248 [==============================] - 34s 3ms/step - loss: 0.3919 - acc: 0.8510 - val_loss: 0.5447 - val_acc: 0.8016\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.51559\n",
      "Epoch 9/15\n",
      "10248/10248 [==============================] - 36s 4ms/step - loss: 0.3625 - acc: 0.8617 - val_loss: 0.5355 - val_acc: 0.7964\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.51559\n",
      "Epoch 10/15\n",
      "10248/10248 [==============================] - 35s 3ms/step - loss: 0.3350 - acc: 0.8694 - val_loss: 0.5571 - val_acc: 0.7960\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.51559\n",
      "Epoch 11/15\n",
      "10248/10248 [==============================] - 34s 3ms/step - loss: 0.3155 - acc: 0.8786 - val_loss: 0.6149 - val_acc: 0.7921\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.51559\n",
      "Epoch 12/15\n",
      "10248/10248 [==============================] - 34s 3ms/step - loss: 0.2983 - acc: 0.8841 - val_loss: 0.5836 - val_acc: 0.7908\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.51559\n",
      "Epoch 13/15\n",
      "10248/10248 [==============================] - 35s 3ms/step - loss: 0.2767 - acc: 0.8970 - val_loss: 0.6468 - val_acc: 0.7869\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.51559\n",
      "Epoch 14/15\n",
      "10248/10248 [==============================] - 34s 3ms/step - loss: 0.2585 - acc: 0.9027 - val_loss: 0.6289 - val_acc: 0.7872\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.51559\n",
      "Epoch 15/15\n",
      "10248/10248 [==============================] - 34s 3ms/step - loss: 0.2442 - acc: 0.9110 - val_loss: 0.6686 - val_acc: 0.7905\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.51559\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x246b2cbaf60>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the model with GloVe\n",
    "model = Sequential() \n",
    "e = Embedding(NB_WORDS, 100, weights=[embedding_matrix], input_length=PADDED_SIZE)#, trainable=False)\n",
    "model.add(e)\n",
    "model.add(SpatialDropout1D(0.5))\n",
    "model.add(LSTM(200, dropout=0.4, recurrent_dropout=0.4)) \n",
    "#model.add(Flatten())\n",
    "model.add(Dense(class_num, activation='softmax')) \n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy']) \n",
    "#model.compile(loss='categorical_crossentropy',optimizer=optimizers.SGD(lr=1e-2, momentum=0.9, nesterov=True), metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train,\n",
    "          Y_train,\n",
    "          validation_data=(X_valid, Y_valid),\n",
    "          epochs=15,\n",
    "          batch_size=BATCH_SIZE,\n",
    "         callbacks=[checkpointer_LSTM_glove, early_stopping, history]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Model without GloVe\n",
    "started with adam but kept having overfitting right from the start, read that i can be\n",
    "because of high learning rate and exploding gradients so i switchd to sgd to try it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_41 (Embedding)     (None, 30, 100)           1493200   \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 30, 128)           25728     \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 30, 64)            16448     \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 30, 32)            4128      \n",
      "_________________________________________________________________\n",
      "conv1d_42 (Conv1D)           (None, 30, 16)            1040      \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 480)               0         \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 480)               0         \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 180)               86580     \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 180)               0         \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 3)                 543       \n",
      "=================================================================\n",
      "Total params: 1,627,667\n",
      "Trainable params: 1,627,667\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 10248 samples, validate on 3074 samples\n",
      "Epoch 1/15\n",
      "10248/10248 [==============================] - 18s 2ms/step - loss: 0.6800 - acc: 0.7185 - val_loss: 0.5581 - val_acc: 0.7733\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.53412\n",
      "Epoch 2/15\n",
      "10248/10248 [==============================] - 14s 1ms/step - loss: 0.4343 - acc: 0.8314 - val_loss: 0.5781 - val_acc: 0.7804\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.53412\n",
      "Epoch 3/15\n",
      "10248/10248 [==============================] - 14s 1ms/step - loss: 0.2822 - acc: 0.8971 - val_loss: 0.7000 - val_acc: 0.7635\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.53412\n",
      "Epoch 4/15\n",
      "10248/10248 [==============================] - 14s 1ms/step - loss: 0.1989 - acc: 0.9279 - val_loss: 0.8630 - val_acc: 0.7612\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.53412\n",
      "Epoch 5/15\n",
      "10248/10248 [==============================] - 14s 1ms/step - loss: 0.1434 - acc: 0.9498 - val_loss: 1.0051 - val_acc: 0.7544\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.53412\n",
      "Epoch 6/15\n",
      "10248/10248 [==============================] - 14s 1ms/step - loss: 0.1182 - acc: 0.9575 - val_loss: 1.2080 - val_acc: 0.7502\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.53412\n",
      "Epoch 7/15\n",
      "10248/10248 [==============================] - 15s 1ms/step - loss: 0.1093 - acc: 0.9608 - val_loss: 1.2767 - val_acc: 0.7430\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.53412\n",
      "Epoch 8/15\n",
      "10248/10248 [==============================] - 14s 1ms/step - loss: 0.0926 - acc: 0.9685 - val_loss: 1.2558 - val_acc: 0.7469\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.53412\n",
      "Epoch 9/15\n",
      "10248/10248 [==============================] - 16s 2ms/step - loss: 0.0852 - acc: 0.9704 - val_loss: 1.5843 - val_acc: 0.7336\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.53412\n",
      "Epoch 10/15\n",
      "10248/10248 [==============================] - 16s 2ms/step - loss: 0.0860 - acc: 0.9717 - val_loss: 1.4863 - val_acc: 0.7378\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.53412\n",
      "Epoch 11/15\n",
      "10248/10248 [==============================] - 14s 1ms/step - loss: 0.0761 - acc: 0.9756 - val_loss: 1.2956 - val_acc: 0.7326\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.53412\n",
      "Epoch 12/15\n",
      "10248/10248 [==============================] - 15s 1ms/step - loss: 0.0640 - acc: 0.9770 - val_loss: 1.6975 - val_acc: 0.7463\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.53412\n",
      "Epoch 13/15\n",
      "10248/10248 [==============================] - 14s 1ms/step - loss: 0.0742 - acc: 0.9757 - val_loss: 1.5300 - val_acc: 0.7349\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.53412\n",
      "Epoch 14/15\n",
      "10248/10248 [==============================] - 14s 1ms/step - loss: 0.0633 - acc: 0.9794 - val_loss: 1.5761 - val_acc: 0.7248\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.53412\n",
      "Epoch 15/15\n",
      "10248/10248 [==============================] - 14s 1ms/step - loss: 0.0682 - acc: 0.9774 - val_loss: 1.4918 - val_acc: 0.7287\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.53412\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x246ccfcf1d0>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the model with self trained embedding layer\n",
    "embedding_vector_length = 100 \n",
    "model = Sequential() \n",
    "model.add(Embedding(NB_WORDS, embedding_vector_length, input_length=PADDED_SIZE)) \n",
    "model.add(Convolution1D(128, 2, padding='same'))\n",
    "model.add(Convolution1D(64, 2, padding='same'))\n",
    "model.add(Convolution1D(32, 2, padding='same'))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(180,activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(class_num, activation='softmax')) \n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy']) \n",
    "#model.compile(loss='categorical_crossentropy',optimizer=optimizers.SGD(lr=1e-2, momentum=0.7, nesterov=True), metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train,\n",
    "          Y_train,\n",
    "          validation_data=(X_valid, Y_valid),\n",
    "          epochs=15,\n",
    "          batch_size=BATCH_SIZE,\n",
    "         callbacks=[checkpointer_CNN, early_stopping, history]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Model with GloVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_49 (Embedding)     (None, 30, 100)           1493200   \n",
      "_________________________________________________________________\n",
      "conv1d_68 (Conv1D)           (None, 30, 32)            6432      \n",
      "_________________________________________________________________\n",
      "conv1d_69 (Conv1D)           (None, 30, 16)            1040      \n",
      "_________________________________________________________________\n",
      "conv1d_70 (Conv1D)           (None, 30, 8)             264       \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 240)               0         \n",
      "_________________________________________________________________\n",
      "dropout_39 (Dropout)         (None, 240)               0         \n",
      "_________________________________________________________________\n",
      "dense_66 (Dense)             (None, 180)               43380     \n",
      "_________________________________________________________________\n",
      "dropout_40 (Dropout)         (None, 180)               0         \n",
      "_________________________________________________________________\n",
      "dense_67 (Dense)             (None, 3)                 543       \n",
      "=================================================================\n",
      "Total params: 1,544,859\n",
      "Trainable params: 1,544,859\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 10248 samples, validate on 3074 samples\n",
      "Epoch 1/15\n",
      "10248/10248 [==============================] - 17s 2ms/step - loss: 0.7699 - acc: 0.6714 - val_loss: 0.6160 - val_acc: 0.7531\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.56676\n",
      "Epoch 2/15\n",
      "10248/10248 [==============================] - 13s 1ms/step - loss: 0.5390 - acc: 0.7826 - val_loss: 0.5883 - val_acc: 0.7589\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.56676\n",
      "Epoch 3/15\n",
      "10248/10248 [==============================] - 13s 1ms/step - loss: 0.3966 - acc: 0.8441 - val_loss: 0.5886 - val_acc: 0.7801\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.56676\n",
      "Epoch 4/15\n",
      "10248/10248 [==============================] - 13s 1ms/step - loss: 0.2945 - acc: 0.8881 - val_loss: 0.6833 - val_acc: 0.7570\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.56676\n",
      "Epoch 5/15\n",
      "10248/10248 [==============================] - 13s 1ms/step - loss: 0.2204 - acc: 0.9172 - val_loss: 0.7449 - val_acc: 0.7651\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.56676\n",
      "Epoch 6/15\n",
      "10248/10248 [==============================] - 13s 1ms/step - loss: 0.1741 - acc: 0.9378 - val_loss: 0.9196 - val_acc: 0.7518\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.56676\n",
      "Epoch 7/15\n",
      "10248/10248 [==============================] - 14s 1ms/step - loss: 0.1308 - acc: 0.9512 - val_loss: 1.0207 - val_acc: 0.7381\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.56676\n",
      "Epoch 8/15\n",
      "10248/10248 [==============================] - 13s 1ms/step - loss: 0.1092 - acc: 0.9594 - val_loss: 1.2705 - val_acc: 0.7450\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.56676\n",
      "Epoch 9/15\n",
      "10248/10248 [==============================] - 13s 1ms/step - loss: 0.0967 - acc: 0.9658 - val_loss: 1.3587 - val_acc: 0.7563\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.56676\n",
      "Epoch 10/15\n",
      "10248/10248 [==============================] - 14s 1ms/step - loss: 0.0841 - acc: 0.9702 - val_loss: 1.3639 - val_acc: 0.7433\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.56676\n",
      "Epoch 11/15\n",
      "10248/10248 [==============================] - 14s 1ms/step - loss: 0.0724 - acc: 0.9741 - val_loss: 1.5213 - val_acc: 0.7450\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.56676\n",
      "Epoch 12/15\n",
      "10248/10248 [==============================] - 14s 1ms/step - loss: 0.0741 - acc: 0.9729 - val_loss: 1.4815 - val_acc: 0.7437\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.56676\n",
      "Epoch 13/15\n",
      "10248/10248 [==============================] - 14s 1ms/step - loss: 0.0537 - acc: 0.9804 - val_loss: 1.7118 - val_acc: 0.7453\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.56676\n",
      "Epoch 14/15\n",
      "10248/10248 [==============================] - 13s 1ms/step - loss: 0.0573 - acc: 0.9815 - val_loss: 1.6112 - val_acc: 0.7420\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.56676\n",
      "Epoch 15/15\n",
      "10248/10248 [==============================] - 13s 1ms/step - loss: 0.0539 - acc: 0.9814 - val_loss: 1.8064 - val_acc: 0.7463\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.56676\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2468835acf8>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the model with self trained embedding layer\n",
    "\n",
    "model = Sequential() \n",
    "e = Embedding(NB_WORDS, 100, weights=[embedding_matrix], input_length=PADDED_SIZE)#, trainable=False)\n",
    "model.add(e)\n",
    "model.add(Convolution1D(32, 2, padding='same'))\n",
    "model.add(Convolution1D(16, 2, padding='same'))\n",
    "model.add(Convolution1D(8, 2, padding='same'))\n",
    "\n",
    "model.add(Flatten())\n",
    "#model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2)) \n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(180,activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(class_num, activation='softmax')) \n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy']) \n",
    "#model.compile(loss='categorical_crossentropy',optimizer=optimizers.SGD(lr=5e-3, momentum=0.9, nesterov=True), metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train,\n",
    "          Y_train,\n",
    "          validation_data=(X_valid, Y_valid),\n",
    "          epochs=15,\n",
    "          batch_size=BATCH_SIZE,\n",
    "         callbacks=[checkpointer_CNN_glove, early_stopping, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM:\n",
      "score: 0.54\n",
      "acc: 0.78\n",
      "--------------------------------\n",
      "LSTM_glove:\n",
      "score: 0.57\n",
      "acc: 0.77\n",
      "--------------------------------\n",
      "CNN:\n",
      "score: 0.59\n",
      "acc: 0.77\n",
      "--------------------------------\n",
      "CNN_glove:\n",
      "score: 0.61\n",
      "acc: 0.76\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "#evaluation test data\n",
    "print(\"LSTM:\")\n",
    "model = load_model(\"weights_LSTM.hdf5\")\n",
    "score,acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size = BATCH_SIZE)\n",
    "print(\"score: %.2f\" % (score))\n",
    "print(\"acc: %.2f\" % (acc))\n",
    "print(\"--------------------------------\")\n",
    "\n",
    "print(\"LSTM_glove:\")\n",
    "model = load_model(\"weights_LSTM_glove.hdf5\")\n",
    "score,acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size = BATCH_SIZE)\n",
    "print(\"score: %.2f\" % (score))\n",
    "print(\"acc: %.2f\" % (acc))\n",
    "print(\"--------------------------------\")\n",
    "\n",
    "print(\"CNN:\")\n",
    "model = load_model(\"weights_CNN.hdf5\")\n",
    "score,acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size = BATCH_SIZE)\n",
    "print(\"score: %.2f\" % (score))\n",
    "print(\"acc: %.2f\" % (acc))\n",
    "print(\"--------------------------------\")\n",
    "\n",
    "print(\"CNN_glove:\")\n",
    "model = load_model(\"weights_CNN_glove.hdf5\")\n",
    "score,acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size = BATCH_SIZE)\n",
    "print(\"score: %.2f\" % (score))\n",
    "print(\"acc: %.2f\" % (acc))\n",
    "print(\"--------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test strings you find on Twitter\n",
    "Here I tested some tweets I gathered manually from the #AmericanAirlines mentions, here is the link: https://twitter.com/hashtag/americanairlines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['negative' 'neutral' 'positive']\n",
      "[0.90746015 0.05815511 0.03438473]\n",
      "tweet was: \n",
      "lost my luggage in AUGUST. They called YESTERDAY to say it was found but was quarantined b/c it was contaminated with bugs in their storage. They said they would be delivering it to our house.\n",
      "the tweett evaluated to: negative\n",
      "---------------------------------------------\n",
      "[0.5718236  0.19081207 0.23736438]\n",
      "tweet was: \n",
      "So, leaving Florida at 10:50pm tonight ( supposedly) getting into Charlotte only to be put in a hotel ( which they better pay for) and flying out at 7:51am. #AmericanAirlines SUCKS once again\n",
      "the tweett evaluated to: negative\n",
      "---------------------------------------------\n",
      "[0.9957956  0.00307553 0.00112893]\n",
      "tweet was: \n",
      "how does #americanairlines cancel my flight and they refuse to refund me? @AmericanAir Worst customer service. Been trying for a week to get a refund.\n",
      "the tweett evaluated to: negative\n",
      "---------------------------------------------\n",
      "[5.9922837e-04 6.7070345e-03 9.9269378e-01]\n",
      "tweet was: \n",
      "Great night at the miamiheat game hanging with my great friend @Lundy2010 Thank you @dwyanewade for dropping by for a picture\n",
      "the tweett evaluated to: positive\n",
      "---------------------------------------------\n",
      "[0.41547307 0.29023656 0.2942904 ]\n",
      "tweet was: \n",
      "Guys, my plane is going to take off into the sunrise and I’ve never been so happy! \n",
      "the tweett evaluated to: negative\n",
      "---------------------------------------------\n",
      "[0.79401416 0.14360952 0.06237637]\n",
      "tweet was: \n",
      "#AmericanAirlines clearly cares way more about keeping my money than about actually serving customers.\n",
      "the tweett evaluated to: negative\n",
      "---------------------------------------------\n",
      "[0.23053204 0.36965844 0.3998096 ]\n",
      "tweet was: \n",
      "Flight diverted for a medical emergency 2 rows behind me.  Good job to the crew for acting quickly on flight 2673 from Orlando. #americanairlines @AmericanAir\n",
      "the tweett evaluated to: positive\n",
      "---------------------------------------------\n",
      "[5.2582533e-03 6.8271271e-04 9.9405909e-01]\n",
      "tweet was: \n",
      "Great job by Cindy Stone ticketing agent at PDX. Getting me routed to St Louis hours earlier   thanks for great customer service. #AmericanAirlines @AmericanAir\n",
      "the tweett evaluated to: positive\n",
      "---------------------------------------------\n",
      "[0.23723105 0.5678169  0.19495201]\n",
      "tweet was: \n",
      "\n",
      "the tweett evaluated to: neutral\n",
      "---------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = load_model(\"weights_LSTM_glove.hdf5\")\n",
    "docs = [\"lost my luggage in AUGUST. They called YESTERDAY to say it was found but was quarantined b/c it was contaminated with bugs in their storage. They said they would be delivering it to our house.\",\n",
    "       \"So, leaving Florida at 10:50pm tonight ( supposedly) getting into Charlotte only to be put in a hotel ( which they better pay for) and flying out at 7:51am. #AmericanAirlines SUCKS once again\",\n",
    "       \"how does #americanairlines cancel my flight and they refuse to refund me? @AmericanAir Worst customer service. Been trying for a week to get a refund.\",\n",
    "       \"Great night at the miamiheat game hanging with my great friend @Lundy2010 Thank you @dwyanewade for dropping by for a picture\",\n",
    "       \"Guys, my plane is going to take off into the sunrise and I’ve never been so happy! \",\n",
    "       \"#AmericanAirlines clearly cares way more about keeping my money than about actually serving customers.\",\n",
    "       \"Flight diverted for a medical emergency 2 rows behind me.  Good job to the crew for acting quickly on flight 2673 from Orlando. #americanairlines @AmericanAir\",\n",
    "       \"Great job by Cindy Stone ticketing agent at PDX. Getting me routed to St Louis hours earlier   thanks for great customer service. #AmericanAirlines @AmericanAir\",\n",
    "       \"\"]\n",
    "docs_test = []\n",
    "for text in docs:\n",
    "    docs_test.append(remove_stopwords(text))\n",
    "\n",
    "docs_encoded_test = [one_hot(d,\n",
    "                        NB_WORDS,\n",
    "                        filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                        lower=True) for d in docs_test]\n",
    "docs_encoded_test = np.asarray(docs_encoded_test)\n",
    "\n",
    "print(le.classes_)\n",
    "classes = le.classes_\n",
    "#for c in classes:\n",
    "#    print(c)\n",
    "#    print(df[ df['sentiment'] == c].size)\n",
    "#    print(\"------------------------------\")\n",
    "    \n",
    "X_new = sequence.pad_sequences(docs_encoded_test, maxlen=PADDED_SIZE)\n",
    "\n",
    "results = model.predict(X_new,batch_size=1,verbose=2)\n",
    "i = 0\n",
    "for result in results:\n",
    "    print(result)\n",
    "    max_index = np.argmax(result)\n",
    "    print(\"tweet was: \")\n",
    "    print(docs[i])\n",
    "    print(\"the tweett evaluated to: \" + str(classes[max_index]))\n",
    "    print(\"---------------------------------------------\")\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
